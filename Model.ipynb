{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.read_csv(\"Dataset/True.csv\")\n",
    "df_false = pd.read_csv(\"Dataset/Fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True News :  21417\n",
      "False News :  23481\n",
      "Dataset :  44898\n"
     ]
    }
   ],
   "source": [
    "#Add category\n",
    "df_true[\"Category\"] = 1\n",
    "df_false[\"Category\"] = 0\n",
    "print(\"True News : \", df_true.title.count())\n",
    "print(\"False News : \", df_false.title.count())\n",
    "#Merge into one dataset\n",
    "df_dataset = pd.concat([df_true, df_false])\n",
    "print(\"Dataset : \", df_dataset.title.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44898"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset[\"Body\"] = df_dataset[\"title\"] + df_dataset[\"text\"]\n",
    "df_dataset.head()\n",
    "\n",
    "del df_dataset[\"text\"]\n",
    "del df_dataset[\"title\"]\n",
    "del df_dataset[\"subject\"]\n",
    "del df_dataset[\"date\"]\n",
    "\n",
    "df_dataset.Body.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                               Body\n",
       "0         1  As U.S. budget fight looms, Republicans flip t...\n",
       "1         1  U.S. military to accept transgender recruits o...\n",
       "2         1  Senior U.S. Republican senator: 'Let Mr. Muell...\n",
       "3         1  FBI Russia probe helped by Australian diplomat...\n",
       "4         1  Trump wants Postal Service to charge 'much mor..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud,STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'couldn', 'no', \"needn't\", 'aren', \"shan't\", 'did', 'just', 'on', 'again', 'very', 'll', 'nor', 'weren', 'their', 'haven', 'ourselves', 'after', 'each', 'where', 'further', 'all', 've', 'yourselves', \"you'd\", 'now', 'as', \"didn't\", 'there', 'mightn', 'isn', 'yourself', 'up', 'more', 'your', 'hasn', 'from', 'don', 'will', 'theirs', 'and', 'above', \"hasn't\", 'having', 'ain', \"wasn't\", 'while', 'by', \"weren't\", 'o', 'shan', 'with', 'wouldn', 'other', \"mightn't\", 'her', 'any', 'me', 'doesn', 'ours', \"wouldn't\", 'she', 'themselves', 'once', 'that', 'am', 'is', 'than', \"you're\", 'myself', 'below', 'only', 'too', \"won't\", 'hadn', 'some', 'out', 'when', 'before', 'not', 'them', 'through', 'how', 'the', 'which', \"it's\", 'himself', \"doesn't\", 'these', 'was', 'such', 'hers', 't', \"haven't\", 'should', 'those', 'against', \"don't\", 'a', 'do', 'of', 'm', 'yours', 'most', 'didn', 'our', 'you', 'had', 'been', 'it', 'both', \"should've\", 'this', \"that'll\", 'during', 'off', 'mustn', \"you've\", 'same', 'so', 'they', 'wasn', 'an', 'or', 'were', 'has', 'ma', 'needn', 'd', 'we', 'shouldn', 'to', 'can', 'does', 'until', 'few', 'its', 'under', \"she's\", 'are', 'for', 'what', 'whom', 'own', 'about', 'into', 'be', 'because', 'over', 'down', 'he', \"hadn't\", 'itself', \"isn't\", 'his', \"mustn't\", 'being', 'then', 'who', 'here', 'my', 'if', 're', 's', 'doing', 'y', 'herself', \"couldn't\", 'at', 'why', \"you'll\", 'have', 'him', \"shouldn't\", \"aren't\", 'won', 'between', 'in', 'i', 'but'}\n"
     ]
    }
   ],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        As U.S. budget fight looms, Republicans flip t...\n",
      "1        U.S. military to accept transgender recruits o...\n",
      "2        Senior U.S. Republican senator: 'Let Mr. Muell...\n",
      "3        FBI Russia probe helped by Australian diplomat...\n",
      "4        Trump wants Postal Service to charge 'much mor...\n",
      "5        White House, Congress prepare for talks on spe...\n",
      "6        Trump says Russia probe will be fair, but time...\n",
      "7        Factbox: Trump on Twitter (Dec 29) - Approval ...\n",
      "8        Trump on Twitter (Dec 28) - Global WarmingThe ...\n",
      "9        Alabama official to certify Senator-elect Jone...\n",
      "10       Jones certified U.S. Senate winner despite Moo...\n",
      "11       New York governor questions the constitutional...\n",
      "12       Factbox: Trump on Twitter (Dec 28) - Vanity Fa...\n",
      "13       Trump on Twitter (Dec 27) - Trump, Iraq, Syria...\n",
      "14       Man says he delivered manure to Mnuchin to pro...\n",
      "15       Virginia officials postpone lottery drawing to...\n",
      "16       U.S. lawmakers question businessman at 2016 Tr...\n",
      "17       Trump on Twitter (Dec 26) - Hillary Clinton, T...\n",
      "18       U.S. appeals court rejects challenge to Trump ...\n",
      "19       Treasury Secretary Mnuchin was sent gift-wrapp...\n",
      "20       Federal judge partially lifts Trump's latest r...\n",
      "21       Exclusive: U.S. memo weakens guidelines for pr...\n",
      "22       Trump travel ban should not apply to people wi...\n",
      "23       Second court rejects Trump bid to stop transge...\n",
      "24       Failed vote to oust president shakes up Peru's...\n",
      "25       Trump signs tax, government spending bills int...\n",
      "26       Companies have up to a year for new U.S. tax b...\n",
      "27       Trump on Twitter (Dec 22) - Tax cut, Missile d...\n",
      "28       Mexico to review need for tax changes after U....\n",
      "29       Senate leader McConnell sees a more collegial ...\n",
      "                               ...                        \n",
      "23451    3.57 Degrees: Kevin Bacon’s Cultural Mantle Sh...\n",
      "23452    Bernie Sanders Could End Up Winning Iowa21st C...\n",
      "23453    Plastic Persona: Behind the Scenes of the Ted ...\n",
      "23454    ‘Meet Jeb’ – Going For Your Sympathy Vote21st ...\n",
      "23455    BOILER ROOM – Examination, Exclamation, Excita...\n",
      "23456    Eyewash: CIA Elites Misleading Employees Indic...\n",
      "23457    Activist: ‘This is where you can make the most...\n",
      "23458    Episode #120 – SUNDAY WIRE: ‘Crisis of Liberty...\n",
      "23459    FBI Release Oregon Video Footage Depicting Dea...\n",
      "23460    Trial By YouTube: Mainstream Media Use Second-...\n",
      "23461    REPORT: ‘Federal Government Escalated the Viol...\n",
      "23462    BOILER ROOM – Oregon Standoff, Cuddle Parties,...\n",
      "23463    Eyewitness Says Feds Ambushed Bundys, 100 Shot...\n",
      "23464    Episode #119 – SUNDAY WIRE: ‘You Know the Dril...\n",
      "23465    ‘There’ll be boots on the ground’: US making n...\n",
      "23466    Boston Brakes? How to Hack a New Car With Your...\n",
      "23467    Oregon Governor Says Feds ‘Must Act’ Against P...\n",
      "23468    Ron Paul on Burns Oregon Standoff and Jury Nul...\n",
      "23469    BOILER ROOM: As the Frogs Slowly Boil – EP #40...\n",
      "23470    Arizona Rancher Protesting in Oregon is Target...\n",
      "23471    Seven Iranians freed in the prisoner swap have...\n",
      "23472    #Hashtag Hell & The Fake Left By Dady Chery an...\n",
      "23473    Astroturfing: Journalist Reveals Brainwashing ...\n",
      "23474    The New American Century: An Era of FraudPaul ...\n",
      "23475    Hillary Clinton: ‘Israel First’ (and no peace ...\n",
      "23476    McPain: John McCain Furious That Iran Treated ...\n",
      "23477    JUSTICE? Yahoo Settles E-mail Privacy Class-ac...\n",
      "23478    Sunnistan: US and Allied ‘Safe Zone’ Plan to T...\n",
      "23479    How to Blow $700 Million: Al Jazeera America F...\n",
      "23480    10 U.S. Navy Sailors Held by Iranian Military ...\n",
      "Name: Body, Length: 44898, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44898"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset[\"Body\"]\n",
    "print(df_dataset[\"Body\"])\n",
    "df_dataset[\"Body\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    text = text.replace('[^\\w\\s]','')\n",
    "    text = text.replace(r\"[\\\"\\',]\", '')\n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    punctuation = list(string.punctuation)\n",
    "    stop.update(punctuation)\n",
    "    text = text.replace('[^\\w\\s]','')\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "#         print(i.strip().lower())\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arkasarkar/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:335: UserWarning: \"https://100percentfedup.com/video-hillary-asked-about-trump-i-just-want-to-eat-some-pie/https://100percentfedup.com/video-hillary-asked-about-trump-i-just-want-to-eat-some-pie/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/arkasarkar/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:335: UserWarning: \"https://fedup.wpengine.com/wp-content/uploads/2015/04/hillarystreetart.jpghttps://fedup.wpengine.com/wp-content/uploads/2015/04/hillarystreetart.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/arkasarkar/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:335: UserWarning: \"https://fedup.wpengine.com/wp-content/uploads/2015/04/entitled.jpghttps://fedup.wpengine.com/wp-content/uploads/2015/04/entitled.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    U.S. budget fight looms, Republicans flip fisc...\n",
      "0    Donald Trump Sends Embarrassing New Year’s Eve...\n",
      "Name: Body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_dataset['Body'] = df_dataset['Body'].apply(clean_text)\n",
    "print(df_dataset[\"Body\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df_dataset.Body, df_dataset.Category, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    U.S. budget fight looms, Republicans flip fisc...\n",
      "0    Donald Trump Sends Embarrassing New Year’s Eve...\n",
      "Name: Body, dtype: object\n",
      "13888    RUBIO Sides Democrats Giving Whopping $2 Billi...\n",
      "7763     Ted Cruz Says Endorsing Trump ‘Grave Mistake’,...\n",
      "8388     ‘Responsible Gun Owner’ Road Rages, Threatens ...\n",
      "16820    TEACHER’S UNION PROTEST Gets Ugly Protesters C...\n",
      "5099     Etihad advises checks U.S. missions new Trump ...\n",
      "5812     Factbox: Trump U.S. Supreme Court pick could a...\n",
      "2747     Trump asks Supreme Court block travel ban ruli...\n",
      "13264    BREAKING BOMBSHELL: Dem Congressmen Phone Numb...\n",
      "12157    Security tight Germany marks anniversary Chris...\n",
      "10120    ALT-LEFT Fake News Media Refuses Tell media tr...\n",
      "15962    Russia hand suspect Montenegro coup attempt: R...\n",
      "8222     U.S. House Speaker Ryan: Pacific trade deal fa...\n",
      "18642    “This TOTALLY FAKE!” Tucker Loses Temper Russi...\n",
      "3151     Trump likely nominate former Senate aide Peirc...\n",
      "22120    MASS INTEGRATION: Race Capitalize Virtual Futu...\n",
      "11046    Obama host Colombian President Santos next wee...\n",
      "23094    FLASHBACK: ‘The Syrian War: You’re Told’ (Abou...\n",
      "16719    Syria's Assad meets dissident footballers Dama...\n",
      "17252    FIND SENATOR VOTED HELP OBAMA Fundamental Tran...\n",
      "1646     Obama CRUSHED Trump’s Lies “Winning” Presidenc...\n",
      "7503     Democrats, civil rights groups disagree levels...\n",
      "12940    Italy's former PM Renzi loses allies election ...\n",
      "12574    ILLEGAL IMMIGRANT VOTING Legal Possibility Cit...\n",
      "9575     Republican Cornyn predicts party unify around ...\n",
      "14986    Exclusive: $6 38 days work: Child exploitation...\n",
      "5739     Senate Budget Committee approves Mulvaney budg...\n",
      "20831    Macron expects casualties, Hurricane Irma hits...\n",
      "21752    CRIES RACISM NYC MUSEUM KICKS ROWDY HIGH SCHOO...\n",
      "9778     White House urges congressional action amid Fl...\n",
      "15400    Kremlin warns mutual damage Ukraine cuts diplo...\n",
      "                               ...                        \n",
      "17083    U.N. says still determining Myanmar crisis gen...\n",
      "10956    BLOCKBUSTER COURT RULING Obama/Clinton Benghaz...\n",
      "8723     U.S. officials working restore public trust po...\n",
      "9764     LADY GAGA BLAMES President Trump Paul Ryan #La...\n",
      "11184    OBAMA ROB FANNIE FREDDIE BILLIONS? Dr. Ben Car...\n",
      "16798    OBAMA’S RADICAL DHS Chief Vows “Protect” Musli...\n",
      "4410     Factbox: Issues stake Trump-China summit Flori...\n",
      "8656     Judge tentatively rejects bid toss Trump Unive...\n",
      "14299    WHOA! SOROS-LINKED GROUP Funding Ohio’s John K...\n",
      "19684    U.N. starting gather testimony Myanmar violati...\n",
      "10588    Trump's immigration outrage poses challenge Cr...\n",
      "4389     Even Undocumented Immigrants Pay Income Taxes ...\n",
      "12011    EPIC TUCKER CARLSON! Liberal Hack Demolished C...\n",
      "1522     U.S. seeks 27 months prison ex-Congressman Ant...\n",
      "15903    Khamenei says Iran, Russia cooperate isolate U...\n",
      "17765    WOW! LOOK PAID Russia Dossier Triggered FBI In...\n",
      "14960    JOKE DAYMy wife went town visited shop.When ca...\n",
      "21149    Hollywood Lefty Leo DiCaprio Goes Rails Climat...\n",
      "21857    (VIDEO)ICE PROTECTING OBAMA: WON’T RELEASE NAM...\n",
      "12444    South Korean army says conducted successful he...\n",
      "9915     KID ROCK GOES Colin Kaepernick, Obamacare Dead...\n",
      "10524    NEWT GINGRICH: Obama, TRUMP, Testify Oath Cong...\n",
      "1455     NATO Officials Planning Trump Visit Small Chil...\n",
      "9090     Sanders vows help Clinton beat Trump, keeps ca...\n",
      "4152     Pro-Trump ‘Christian’ Leaders Got Taken Woodsh...\n",
      "16364    MISSING: TWO FILE ‘BOXES’ CLINTON E-MAILS…Evid...\n",
      "2275     Trump Twitter (Aug 7): Fake News, Senator Rich...\n",
      "19324    Ireland says 'lot work' needed move next phase...\n",
      "1178     House panel sets Puerto Rico recovery hearing ...\n",
      "13055    IDENTITY HILLARY’S MYSTERY “HANDLER” Finally R...\n",
      "Name: Body, Length: 8980, dtype: object\n",
      "16971    0\n",
      "1719     0\n",
      "2804     0\n",
      "8837     1\n",
      "15902    1\n",
      "20055    0\n",
      "649      1\n",
      "10413    0\n",
      "10996    1\n",
      "13761    1\n",
      "12190    1\n",
      "20139    1\n",
      "10839    1\n",
      "12783    0\n",
      "14385    0\n",
      "6631     0\n",
      "10677    0\n",
      "18058    0\n",
      "4556     0\n",
      "12166    0\n",
      "22063    0\n",
      "14353    1\n",
      "22509    0\n",
      "4831     1\n",
      "15834    1\n",
      "8397     1\n",
      "14764    1\n",
      "7148     0\n",
      "14098    0\n",
      "3986     1\n",
      "        ..\n",
      "7599     1\n",
      "1871     1\n",
      "18430    1\n",
      "7877     1\n",
      "16202    0\n",
      "5072     1\n",
      "2163     1\n",
      "17387    0\n",
      "6921     1\n",
      "17567    0\n",
      "6052     0\n",
      "16921    1\n",
      "14248    0\n",
      "2735     0\n",
      "21678    0\n",
      "18983    1\n",
      "10813    0\n",
      "17089    1\n",
      "14650    1\n",
      "18095    0\n",
      "15430    1\n",
      "14935    1\n",
      "20757    1\n",
      "20576    0\n",
      "10686    0\n",
      "8986     0\n",
      "21243    1\n",
      "21196    0\n",
      "22150    0\n",
      "2732     1\n",
      "Name: Category, Length: 35918, dtype: int64\n",
      "13888    0\n",
      "7763     0\n",
      "8388     0\n",
      "16820    0\n",
      "5099     1\n",
      "5812     1\n",
      "2747     1\n",
      "13264    0\n",
      "12157    1\n",
      "10120    0\n",
      "15962    1\n",
      "8222     1\n",
      "18642    0\n",
      "3151     1\n",
      "22120    0\n",
      "11046    1\n",
      "23094    0\n",
      "16719    1\n",
      "17252    0\n",
      "1646     0\n",
      "7503     1\n",
      "12940    1\n",
      "12574    0\n",
      "9575     1\n",
      "14986    1\n",
      "5739     1\n",
      "20831    1\n",
      "21752    0\n",
      "9778     1\n",
      "15400    1\n",
      "        ..\n",
      "17083    1\n",
      "10956    0\n",
      "8723     1\n",
      "9764     0\n",
      "11184    0\n",
      "16798    0\n",
      "4410     1\n",
      "8656     1\n",
      "14299    0\n",
      "19684    1\n",
      "10588    1\n",
      "4389     0\n",
      "12011    0\n",
      "1522     1\n",
      "15903    1\n",
      "17765    0\n",
      "14960    0\n",
      "21149    0\n",
      "21857    0\n",
      "12444    1\n",
      "9915     0\n",
      "10524    0\n",
      "1455     0\n",
      "9090     1\n",
      "4152     0\n",
      "16364    0\n",
      "2275     1\n",
      "19324    1\n",
      "1178     1\n",
      "13055    0\n",
      "Name: Category, Length: 8980, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(X_test)\n",
    "print(Y_train)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.array(X_train)\n",
    "# Y_train = np.array(Y_train)\n",
    "# X_test = np.array(X_test)\n",
    "# Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35918,)\n",
      "(35918,)\n",
      "(8980,)\n",
      "(8980,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "# print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
    "# print(tokenized_train)\n",
    "padded = pad_sequences(tokenized_train, maxlen = 300)\n",
    "X_train = padded\n",
    "tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(tokenized_test, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 3082  813  295]\n",
      " [   0    0    0 ... 3925  481  396]\n",
      " [   0    0    0 ... 2895  481  396]\n",
      " ...\n",
      " [   0    0    0 ...   76 2710 1405]\n",
      " [2124 8239 4190 ...  288 1066  642]\n",
      " [   0    0    0 ... 3347  755 4497]]\n",
      "[[   0    0    0 ... 2992  403  673]\n",
      " [   0    0    0 ...  109   76 3222]\n",
      " [   0    0    0 ...  109   76 3355]\n",
      " ...\n",
      " [   0    0    0 ... 3172 2121 1710]\n",
      " [   0    0    0 ...  260 3067  747]\n",
      " [   0    0    0 ... 3304  233 1023]]\n",
      "(35918, 300)\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(X_test)\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#its important to use the glove.twitter.27B.100d.txt embeddings \n",
    "# embedding_dict = {}\n",
    "# with open(\"/home/arkasarkar/Desktop/DL_projects/glove.twitter.27B.100d.txt\", 'r', encoding = \"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]\n",
    "#         vector = np.asarray(values[1:], \"float32\")\n",
    "# #         print(word,vector)\n",
    "#         embedding_dict[word] = vector\n",
    "\n",
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(\"/home/arkasarkar/Desktop/DL_projects/glove.twitter.27B.100d.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.24645    0.45248    0.8372     0.1301    -0.10773   -0.23649\n",
      " -0.022622  -0.19291    0.0089398 -0.42653    0.46929   -0.2226\n",
      " -1.5809    -0.50369    0.302     -0.032982  -0.82088   -0.30826\n",
      "  0.32375    0.079145   0.45206   -0.60606   -0.021866   0.33313\n",
      " -0.023317   0.44279    0.47152   -0.037821  -0.072843  -0.72901\n",
      "  0.65716    0.19021   -0.34567   -0.71308    0.034026   0.30534\n",
      " -0.21855    0.024003  -0.14556   -0.68436    0.93626    0.12909\n",
      " -0.17345   -0.68616    0.011613   0.32246   -0.48218   -0.12962\n",
      " -0.12519    0.36592   -0.058803  -0.99927   -0.39455    0.29715\n",
      " -0.44197    0.16283    0.17432   -0.74203   -0.41865    0.21203\n",
      "  0.18244   -0.020944   0.051607  -0.17384    0.16151   -0.39944\n",
      "  0.52242    0.55355   -0.70574    0.1701    -0.22219   -0.61542\n",
      " -0.31061   -0.29061    0.10021   -0.058518   0.19118    0.8167\n",
      "  0.092544   0.25594    0.8109    -0.14547   -0.25681    0.37606\n",
      " -0.74676   -0.41652    0.29502   -0.39016   -0.55615   -0.50449\n",
      " -0.55999   -0.63991    0.0030587 -0.29089    0.070067   0.39176\n",
      " -0.55107   -0.98968   -0.25141   -0.54979  ]\n"
     ]
    }
   ],
   "source": [
    "print((embeddings_index[\"trump\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arkasarkar/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "#change below line if computing normal stats is too slow\n",
    "embedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCURACY_THRESHOLD = 0.98\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('acc') > ACCURACY_THRESHOLD):\n",
    "            print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(ACCURACY_THRESHOLD*100))\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def step_decay(epochs):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_weights-{epoch:02d}-{val_accuracy:.2f}.hdf5\", monitor='loss', verbose=1,save_best_only=True, mode='auto', period=1)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arkasarkar/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4081: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "unified_lstm (UnifiedLSTM)   (None, 300, 128)          117248    \n",
      "_________________________________________________________________\n",
      "unified_lstm_1 (UnifiedLSTM) (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,168,769\n",
      "Trainable params: 168,769\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(max_features, output_dim = embedding_matrix.shape[1], weights = [embedding_matrix], input_length = 300, trainable = False),\n",
    "#     LSTM(256, return_sequences = True),\n",
    "    LSTM(128, return_sequences = True, recurrent_dropout = 0.25 , dropout = 0.25),\n",
    "    LSTM(64, recurrent_dropout = 0.1 , dropout = 0.1),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dense(1, activation = 'sigmoid')   \n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 0.01, beta_1=0.9, beta_2=0.999,epsilon=1e-07), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35918 samples, validate on 8980 samples\n",
      "Epoch 1/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.3939 - accuracy: 0.8158\n",
      "Epoch 00001: loss improved from inf to 0.39388, saving model to model_weights-01-0.87.hdf5\n",
      "35918/35918 [==============================] - 351s 10ms/sample - loss: 0.3939 - accuracy: 0.8157 - val_loss: 0.2916 - val_accuracy: 0.8718\n",
      "Epoch 2/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.2604 - accuracy: 0.8896\n",
      "Epoch 00002: loss improved from 0.39388 to 0.26006, saving model to model_weights-02-0.93.hdf5\n",
      "35918/35918 [==============================] - 345s 10ms/sample - loss: 0.2601 - accuracy: 0.8898 - val_loss: 0.1707 - val_accuracy: 0.9314\n",
      "Epoch 3/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9650\n",
      "Epoch 00003: loss improved from 0.26006 to 0.09202, saving model to model_weights-03-0.98.hdf5\n",
      "35918/35918 [==============================] - 243s 7ms/sample - loss: 0.0920 - accuracy: 0.9651 - val_loss: 0.0393 - val_accuracy: 0.9833\n",
      "Epoch 4/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9883\n",
      "Epoch 00004: loss improved from 0.09202 to 0.03343, saving model to model_weights-04-0.99.hdf5\n",
      "35918/35918 [==============================] - 244s 7ms/sample - loss: 0.0334 - accuracy: 0.9883 - val_loss: 0.0241 - val_accuracy: 0.9909\n",
      "Epoch 5/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.0307 - accuracy: 0.9894\n",
      "Epoch 00005: loss improved from 0.03343 to 0.03071, saving model to model_weights-05-0.99.hdf5\n",
      "35918/35918 [==============================] - 292s 8ms/sample - loss: 0.0307 - accuracy: 0.9894 - val_loss: 0.0201 - val_accuracy: 0.9931\n",
      "Epoch 6/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.0221 - accuracy: 0.9917\n",
      "Epoch 00006: loss improved from 0.03071 to 0.02237, saving model to model_weights-06-0.99.hdf5\n",
      "35918/35918 [==============================] - 288s 8ms/sample - loss: 0.0224 - accuracy: 0.9916 - val_loss: 0.0197 - val_accuracy: 0.9934\n",
      "Epoch 7/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.0228 - accuracy: 0.9919\n",
      "Epoch 00007: loss did not improve from 0.02237\n",
      "35918/35918 [==============================] - 266s 7ms/sample - loss: 0.0231 - accuracy: 0.9918 - val_loss: 0.0246 - val_accuracy: 0.9904\n",
      "Epoch 8/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.4689 - accuracy: 0.7143\n",
      "Epoch 00008: loss did not improve from 0.02237\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "35918/35918 [==============================] - 299s 8ms/sample - loss: 0.4694 - accuracy: 0.7136 - val_loss: 0.6925 - val_accuracy: 0.5200\n",
      "Epoch 9/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5238\n",
      "Epoch 00009: loss did not improve from 0.02237\n",
      "35918/35918 [==============================] - 349s 10ms/sample - loss: 0.6920 - accuracy: 0.5238 - val_loss: 0.6919 - val_accuracy: 0.5200\n",
      "Epoch 10/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5237\n",
      "Epoch 00010: loss did not improve from 0.02237\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "35918/35918 [==============================] - 363s 10ms/sample - loss: 0.6919 - accuracy: 0.5237 - val_loss: 0.6921 - val_accuracy: 0.5200\n",
      "Epoch 11/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5236\n",
      "Epoch 00011: loss did not improve from 0.02237\n",
      "35918/35918 [==============================] - 359s 10ms/sample - loss: 0.6913 - accuracy: 0.5237 - val_loss: 0.6892 - val_accuracy: 0.5200\n",
      "Epoch 12/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.6898 - accuracy: 0.5261\n",
      "Epoch 00012: loss did not improve from 0.02237\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "35918/35918 [==============================] - 357s 10ms/sample - loss: 0.6898 - accuracy: 0.5261 - val_loss: 0.6808 - val_accuracy: 0.5698\n",
      "Epoch 13/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5604\n",
      "Epoch 00013: loss did not improve from 0.02237\n",
      "35918/35918 [==============================] - 354s 10ms/sample - loss: 0.6846 - accuracy: 0.5605 - val_loss: 0.6742 - val_accuracy: 0.5836\n",
      "Epoch 14/20\n",
      "35840/35918 [============================>.] - ETA: 0s - loss: 0.6772 - accuracy: 0.5807\n",
      "Epoch 00014: loss did not improve from 0.02237\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "35918/35918 [==============================] - 331s 9ms/sample - loss: 0.6772 - accuracy: 0.5806 - val_loss: 0.6678 - val_accuracy: 0.5813\n",
      "Epoch 15/20\n",
      " 9728/35918 [=======>......................] - ETA: 3:18 - loss: 0.6717 - accuracy: 0.5883"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5d02973caa09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate_reduction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3217\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 415\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train , validation_data = (X_test,Y_test), batch_size = 256 , epochs = 20 , callbacks = [checkpoint,learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35918,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
